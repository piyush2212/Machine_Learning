{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Python\\lib\\site-packages\\sklearn\\externals\\six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "G:\\Python\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import mglearn as mg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sl\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # IRIS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()\n",
    "iris_dataset.keys()\n",
    "print(iris_dataset['DESCR'] + '\\n')\n",
    "print('First Five rows of data : \\n {}'.format(iris_dataset['data'][:5]))\n",
    "iris_dataset['target'].shape\n",
    "iris_dataset['target_names']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Train Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(iris_dataset['data'],iris_dataset['target'],random_state = 0)\n",
    "print(\"X_train shape = {}\".format(X_train.shape))\n",
    "df=pd.DataFrame(X_train,columns = iris_dataset.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting HIST graphs to test data quality\n",
    "\n",
    "pd.plotting.scatter_matrix(df,figsize=(15,15),diagonal='hist',hist_kwds={'bins':20},s=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection by finding outliers through SD,mean\n",
    "\n",
    "anomalies = []\n",
    "def find_anomalies(random_data):\n",
    "    # Set upper and lower limit to 3 standard deviation\n",
    "    random_data_std = np.std(random_data)\n",
    "    random_data_mean = np.mean(random_data)\n",
    "    anomaly_cut_off = random_data_std * 3\n",
    "    \n",
    "    lower_limit  = random_data_mean - anomaly_cut_off \n",
    "    upper_limit = random_data_mean + anomaly_cut_off\n",
    "    # Generate outliers\n",
    "    for outlier in random_data:\n",
    "        if outlier > upper_limit or outlier < lower_limit:\n",
    "            anomalies.append(outlier)\n",
    "    return anomalies,lower_limit,upper_limit \n",
    "\n",
    "find_anomalies(df['sepal length (cm)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection by plotting box plot\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.boxplot(df['petal length (cm)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)   \n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# Making Predictions\n",
    "\n",
    "X_new = np.array([[5,2.9,1,0.2],[7.1,5.0,1,2.9]])\n",
    "prediction = knn.predict(X_new)\n",
    "format(iris_dataset['target_names'][prediction])\n",
    "\n",
    "# Testing accuracy of model\n",
    "\n",
    "prediction = knn.predict(X_test)\n",
    "print('Accuracy : {}'.format(np.mean(prediction == y_test)))\n",
    "knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of data\n",
    "\n",
    "X,y = mg.datasets.make_forge()\n",
    "knn = KNeighborsClassifier(n_neighbors = 3) \n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 0)\n",
    "plt.scatter(X_train[:,0],X_train[:,1],c = y_train, marker = 'o')\n",
    "plt.scatter(X_test[:,0],X_test[:,1],c = y_test,marker = 'v')\n",
    "knn.fit(X_train,y_train)\n",
    "knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Decision Boundary\n",
    "\n",
    "fig, ax = plt.subplots(1,4,figsize = (10,3))\n",
    "from matplotlib.colors import ListedColormap\n",
    "for n,ax in zip([1,3,9,15],ax):\n",
    "    clf = KNeighborsClassifier(n)\n",
    "    create_decision_boundary(clf = clf,X=X,y=y,cmap_light = ListedColormap(['orange', 'cornflowerblue']),\n",
    "                             cmap_bold = ListedColormap(['darkorange','darkblue']),ax = ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression KNN \n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knr = KNeighborsRegressor(1)\n",
    "X,y = mg.datasets.make_wave(n_samples = 40)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 1)\n",
    "\n",
    "# here score calculates R squared\n",
    "for i in np.arange(1,10,1):\n",
    "    knr = KNeighborsRegressor(i)\n",
    "    knr.fit(X_train,y_train)\n",
    "    y = knr.score(X_test,y_test)\n",
    "    z = knr.score(X_train,y_train)\n",
    "    plt.plot(i,y,'bo')\n",
    "    plt.plot(i,z,'rv')\n",
    "\n",
    "# Creating decision boundary for KNN regressor\n",
    "fig, ax = plt.subplots(1,4,figsize = (10,3))\n",
    "z = np.linspace(-3,3,1000).reshape(-1,1)\n",
    "from matplotlib.colors import ListedColormap\n",
    "for n,ax in zip([1,3,9,15],ax):\n",
    "    clf = KNeighborsRegressor(n)\n",
    "    clf.fit(X,y)\n",
    "    ax.plot(z,clf.predict(z))\n",
    "    ax.plot(X,y,'ro')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Boundaries Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decision_boundary(clf,X,y,cmap_light,cmap_bold,ax) :\n",
    "    \n",
    "    X = X[:, :2]\n",
    "    h = .02  # step size in the mesh\n",
    "    clf.fit(X, y)\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training pointsqw\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "                edgecolor='k', s=20)\n",
    "    #ax.xlim(xx.min(), xx.max())\n",
    "    #ax.ylim(yy.min(), yy.max())\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating synthetic classification data\n",
    "\n",
    "X,y = mg.datasets.make_forge()\n",
    "df = pd.DataFrame(X)\n",
    "#sns.boxplot(df[1])\n",
    "#find_anomalies(df[1])\n",
    "df.describe()\n",
    "plt.scatter(df[0],df[1], c = y)\n",
    "plt.legend([\"class 0 \",\"class 1\"],loc =4)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating synthetic regression data\n",
    "\n",
    "X,y = mg.datasets.make_wave(n_samples = 40)\n",
    "plt.plot(X,y,'o')\n",
    "find_anomalies(X)\n",
    "#sns.boxplot(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real world datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancer dataset\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "cancer.data.shape\n",
    "cancer.feature_names\n",
    "#print('\\n {}'.format(cancer.DESCR))\n",
    "cancer.target\n",
    "df = pd.DataFrame(cancer.data,columns = cancer.feature_names)\n",
    "#pd.plotting.scatter_matrix(df,figsize=(15,15),diagonal='hist',hist_kwds={'bins':20},s=60)\n",
    "#plt.show()\n",
    "n,v = zip(cancer.target_names,np.bincount(cancer.target))\n",
    "plt.scatter(df[cancer.feature_names[29]],df[cancer.feature_names[1]],c = cancer.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying kneighbors on breast cancer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "knn = KNeighborsClassifier(n_neighbors = 1) \n",
    "X_train,X_test,y_train,y_test = train_test_split(cancer.data,cancer.target,random_state = 0)\n",
    "knn.fit(X_train,y_train)\n",
    "knn.predict(X_test)\n",
    "knn.score(X_test,y_test)\n",
    "for i in np.arange(1,10,1):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i) \n",
    "    knn.fit(X_train,y_train)\n",
    "    y = knn.score(X_test,y_test)\n",
    "    z = knn.score(X_train,y_train)\n",
    "    plt.plot(i,y,'bo')\n",
    "    plt.plot(i,z,'rv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mg.plots.plot_linear_regression_wave()\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X,y = mg.datasets.make_wave(n_samples = 40)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 0)\n",
    "LR = LinearRegression()\n",
    "type(LR)\n",
    "y = LR.fit(X_train,y_train)\n",
    "y.intercept_\n",
    "y.coef_\n",
    "LR.predict(X_test)\n",
    "z = np.linspace(-5,5,1000).reshape(-1,1)\n",
    "plt.plot(z,LR.predict(z),'b')\n",
    "plt.plot(X_train,y_train,'ro')\n",
    "plt.plot(X_test,y_test,'bo')\n",
    "LR.score(X_test,y_test)\n",
    "LR.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.607472195966589"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Boston Housing Datset \n",
    "\n",
    "X,y = mg.datasets.load_extended_boston()\n",
    "X.shape\n",
    "y.shape\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 0)\n",
    "lrfit = LR.fit(X_train,y_train) \n",
    "lrfit.coef_\n",
    "lrfit.intercept_\n",
    "LR.score(X_train,y_train)\n",
    "LR.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8857966585170941"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "ridge = Ridge()\n",
    "X,y = mg.datasets.load_extended_boston()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 0)\n",
    "z = ridge.fit(X_train,y_train)\n",
    "z.intercept_\n",
    "ridge.score(X_test,y_test)\n",
    "ridge.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing Alpha test\n",
    "\n",
    "import math\n",
    "alpha = [0.00001,0.01,1,10,100,1000]\n",
    "for alpha in alpha:\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train,y_train)\n",
    "    lasso.fit(X_train,y_train)\n",
    "    r1 = ridge.score(X_test,y_test)\n",
    "    r2 = ridge.score(X_train,y_train)\n",
    "    l1 = lasso.score(X_test,y_test)\n",
    "    l2 = lasso.score(X_train,y_train)\n",
    "    plt.plot(math.log10(alpha),r1,'bo')\n",
    "    plt.plot(math.log10(alpha),r2,'b*')\n",
    "    plt.plot(math.log10(alpha),l1,'co')\n",
    "    plt.plot(math.log10(alpha),l2,'c*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficients comparison\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "ridge1 = Ridge(alpha=1)\n",
    "ridge10 = Ridge(alpha=10)\n",
    "ridge0_1 = Ridge(alpha=0.01)\n",
    "LR = LinearRegression()\n",
    "a= ridge1.fit(X_train,y_train)\n",
    "b = ridge10.fit(X_train,y_train)\n",
    "c = ridge0_1.fit(X_train,y_train)\n",
    "d = LR.fit(X_train,y_train)\n",
    "plt.plot(a.coef_,'bo')\n",
    "plt.plot(b.coef_,'ro')\n",
    "plt.plot(c.coef_,'go')\n",
    "plt.plot(d.coef_,'co')\n",
    "plt.ylim(-20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7656571174549983"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lasso Regression\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "ls = Lasso(alpha=0.01,max_iter=100000)\n",
    "p = ls.fit(X_train,y_train)\n",
    "p.coef_\n",
    "p.intercept_\n",
    "ls.score(X_train,y_train)\n",
    "ls.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2756452492089402"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Elastic Net Combination of lasso and ridge\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "EN = ElasticNet(alpha=1,max_iter=10000,l1_ratio=0.0001)\n",
    "EN.fit(X_train,y_train)q23456\n",
    "EN.coef_\n",
    "EN.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Logistic Regresssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X,y = mg.datasets.make_forge()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 0)\n",
    "LG = LogisticRegression(C= 0.1,l1_ratio= 0.5,penalty= 'elasticnet',solver='saga',max_iter = 1000)\n",
    "LG.fit(X_train,y_train)\n",
    "print(LG.score(X_train,y_train))\n",
    "print(LG.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Decision Boundary for LogisticRegression\n",
    "\n",
    "clf = LG.fit(X,y)\n",
    "mg.plots.plot_2d_separator(clf,X,fill = False, eps = 0.5,alpha =0.7)\n",
    "mg.discrete_scatter(X[:,0],X[:,1],y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Support Vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "SVC = LinearSVC(C = 1,max_iter = 10000)\n",
    "SVC.fit(X_train,y_train)\n",
    "SVC.score(X_train,y_train)\n",
    "SVC.score(X_test,y_test)\n",
    "\n",
    "clf = SVC.fit(X,y)\n",
    "mg.plots.plot_2d_separator(clf,X,fill = False, eps = 0.5,alpha =0.7)\n",
    "mg.discrete_scatter(X[:,0],X[:,1],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
